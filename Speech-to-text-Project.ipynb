{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Framework\n",
    "## Speech to Text\n",
    "#### Nouha Kacimi Alaoui, Nour Amine, Scarlett Gatt, Hà Nhi Ngo et Khoa Thi Nguyen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "«Hey Siri », « OK Google » are some quotes that that have been part of our daily life, with the appearance of voice search on our smartphones and laptops. Nowadays, more and more people are using voice recognition to ask, for example, what time it is, the weather of the day, or even sports results. \n",
    "Automatic speech recognition is actually a computer concept that analyzes the voice of a human, in order to transcribe it into text. Behind this computer concept, we can find several algorithms. Among these, we have the Google Speech to Text Algorithm.\n",
    "This algorithm allows developers to convert sound to text by applying powerful neural network models via an easy-to-use API, which recognizes 120 languages. \n",
    "The Cloud Speech-To-Text API uses the most sophisticated deep learning algorithms on the market. This neural network based technology enables speech recognition with unrivaled precision. However, we noticed some transcription errors while testing this algorithm.\n",
    "Although Google has one of the most powerful voice recognition technologies on the market, it struggles to correctly transcribe the words of people who have a pronounced accent, or a difficulty to speak. Thus, in order to simplify, we decided to work on another algorithm that we found in the « Challenges Speech-To-Text Algorithms » section on Kaggle. It is an algorithm which transcribes a vocal comprising only one english word, into text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing the libraries that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "6e7b6f6347f596f78b3a8fb88cb7a820541998b1"
   },
   "outputs": [],
   "source": [
    "#path\n",
    "import os\n",
    "from os.path import isdir, join\n",
    "from pathlib import Path\n",
    "\n",
    "# Scientific Math \n",
    "import numpy as np\n",
    "from scipy.fftpack import fft\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "#Deep learning\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras import Input, layers\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import random\n",
    "import copy\n",
    "import librosa\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bc717705d62e8a2a312a983f59ab0406c0e0329b"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "In this section, we will import our data and gather it into groups for later use.\n",
    "\n",
    "For that, we first extract the names of the folders that contain the sound samples. Note that the names of these folders match the sound in the 'wav' files that they hold. \n",
    "\n",
    "\n",
    "Instruction : You have to change the path leading to your data (`train_audio_path`).\n",
    "\n",
    "We use the `os.listdir` function with the condition (`isdirjoin(train_audio_path, f)`) to return the names of the folders in our path. The condition is here to prevent any file names from entering the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "baf56ce3fd56eb7c6dd4b5d088105a631654071e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels: 30\n",
      "['_background_noise_', 'bed', 'bird', 'cat', 'dog', 'down', 'eight', 'five', 'four', 'go', 'happy', 'house', 'left', 'marvin', 'nine', 'no', 'off', 'on', 'one', 'right', 'seven', 'sheila', 'six', 'stop', 'three', 'tree', 'two', 'up', 'wow', 'yes', 'zero']\n"
     ]
    }
   ],
   "source": [
    "train_audio_path = 'train/audio/'\n",
    "dirs = [f for f in os.listdir(train_audio_path) if isdir(join(train_audio_path, f))]\n",
    "dirs.sort()\n",
    "print('Number of labels: ' + str(len(dirs[1:])))\n",
    "print(dirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " As we have a big amount of data, we gather three samples of sounds :\n",
    " \n",
    " - Target list is the first sample, it contains folders of the following sounds : `'yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go'`.\n",
    " - Unknown list contains all the folders except for the ones in the target list and the ones in the background noise. The folders in this list are : `'bed', 'bird', 'cat', 'dog', 'eight', 'five', 'four', 'happy', 'house', 'marvin', 'nine', 'one', 'seven', 'sheila', 'six', 'three', 'tree', 'two', 'wow', 'zero'`.\n",
    " - The background noise is a list of the sounds in the folder '_background_noise_' ending with '.wav', we only extract sound files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_list : ['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go']\n",
      "unknowns_list : ['bed', 'bird', 'cat', 'dog', 'eight', 'five', 'four', 'happy', 'house', 'marvin', 'nine', 'one', 'seven', 'sheila', 'six', 'three', 'tree', 'two', 'wow', 'zero']\n",
      "silence : _background_noise_\n"
     ]
    }
   ],
   "source": [
    "all_wav = []\n",
    "unknown_wav = []\n",
    "label_all = []\n",
    "label_value = {}\n",
    "target_list = ['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go']\n",
    "unknown_list = [d for d in dirs if d not in target_list and d != '_background_noise_' ]\n",
    "print('target_list : ',end='')\n",
    "print(target_list)\n",
    "print('unknowns_list : ', end='')\n",
    "print(unknown_list)\n",
    "print('silence : _background_noise_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following process concerns the background noises :\n",
    "\n",
    "1- Using librosa.load, we load the audio backgroung noises as a floating point time series. This function returns the sample (audio time series) and the sampling rate of the audio time series. The audio is automatically resampled to the rate sr=22050 ( default).\\\n",
    "\n",
    "\n",
    "2- We use librosa.resample to resample our audio time series from 'sample_rate' ( the default value : 22050 ) to the new_sample rate : 8000Hz.\\\n",
    "\n",
    "\n",
    "3- We add the resampled background noise  to the 'background_noise' list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "background = [f for f in os.listdir(join(train_audio_path, '_background_noise_')) if f.endswith('.wav')]\n",
    "background_noise = []\n",
    "for wav in background : \n",
    "    samples, sample_rate = librosa.load(join(join(train_audio_path,'_background_noise_'),wav))\n",
    "    samples = librosa.resample(samples, sample_rate, 8000)\n",
    "    background_noise.append(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now process the rest of the data :\n",
    "\n",
    "1- We first select all the 'wav' files of each folders (except for the background noise), that we put in the 'waves' list.\\\n",
    "\n",
    "\n",
    "2- We index each folder from 1 to 30 as following : `1:bed 2:bird 3:cat 4:dog 5:down 6:eight 7:five 8:four 9:go 10:happy 11:house 12:left 13:marvin 14:nine 15:no 16:off 17:on 18:one 19:right 20:seven 21:sheila 22:six 23:stop 24:three 25:tree 26:two 27:up 28:wow 29:yes 30:zero`.\\\n",
    "\n",
    "\n",
    "3- The same way than the background noise, we load the audio files. Then, we resample them with a rate of 16000 instead of 22050 to a rate of 8000.\n",
    "\n",
    "\n",
    "4- If the folder is from the unknown list, we add the sample in the unkown_wav list. Else, we add our sample and its folder name in all_wav.\n",
    "\n",
    "The data sampling rate is reduced to 8000Hz to lessen the computation cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "cdeb5698a33395d714bd7ec249f25a668734bc7d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:bed 2:bird 3:cat 4:dog 5:down 6:eight 7:five 8:four 9:go 10:happy 11:house 12:left 13:marvin 14:nine 15:no 16:off 17:on 18:one 19:right 20:seven 21:sheila 22:six 23:stop 24:three 25:tree 26:two 27:up 28:wow 29:yes 30:zero "
     ]
    }
   ],
   "source": [
    "i=0;\n",
    "for direct in dirs[1:]:\n",
    "    waves = [f for f in os.listdir(join(train_audio_path, direct)) if f.endswith('.wav')]\n",
    "    label_value[direct] = i\n",
    "    i = i + 1\n",
    "    print(str(i)+\":\" +str(direct) + \" \", end=\"\")\n",
    "    for wav in waves:\n",
    "        samples, sample_rate = librosa.load(join(join(train_audio_path,direct),wav), sr = 16000)\n",
    "        samples = librosa.resample(samples, sample_rate, 8000)\n",
    "        if len(samples) != 8000 : \n",
    "            continue\n",
    "            \n",
    "        if direct in unknown_list:\n",
    "            unknown_wav.append(samples)\n",
    "        else:\n",
    "            label_all.append(direct)\n",
    "            all_wav.append([samples, direct])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a reminder : \n",
    "\n",
    "- dirs contains all the folder names  .\n",
    "- Target_list : `'yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go']` ( folders ) ( also label_all)\n",
    "- unknowns_list : `'bed', 'bird', 'cat', 'dog', 'eight', 'five', 'four', 'happy', 'house', 'marvin', 'nine', 'one', 'seven', 'sheila', 'six', 'three', 'tree', 'two', 'wow', 'zero'` ( folders)\n",
    "- background_noise the list of the resampled background noises.(samples) \n",
    "- unknown_wav is the list of the resamples noises of the unknowns_list ( samples ) \n",
    "- all_wav is the list of the resampled noises of the target list ( [sample, name of the folder] ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will cut all_wav in two lists : one part, the sample, are going in `wav_all` and the second part, the folder names, are going in `label_all`.\n",
    "In the next step, we divide 'all_wav' into two other lists. The first one, 'Wav_all', will contain the samples and the other one, 'label_all' will contain the folder names (in direct)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "8501a2e50843829e3a51853c683f105b040e954f"
   },
   "outputs": [],
   "source": [
    "wav_all = np.reshape(np.delete(all_wav,1,1),(len(all_wav)))\n",
    "label_all = [i for i in np.delete(all_wav,0,1).tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a3239651e24e6decb63e5c5c9221650357d684c2"
   },
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunatly, we may not have enough data to train correctly our model. Moreover, the audio files are too clean : the people who may wish to translate their words to texts may be bothered by some background sounds. To assure that our model needs to be able to understand them anyway, we are duplicating some audio files and adding hazardous background sound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "b59088b2bb5dc1848246dd4320d134ac0c673e65"
   },
   "outputs": [],
   "source": [
    "#Random pick start point\n",
    "def get_one_noise(noise_num = 0):\n",
    "    selected_noise = background_noise[noise_num]\n",
    "    start_idx = random.randint(0, len(selected_noise)- 1 - 8000)\n",
    "    return selected_noise[start_idx:(start_idx + 8000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A noise is selected to get added to the audio files and put in `noise`. Then, every audio file of `wav_all` see itself either put in the `delete_index`to be deleted later if its length isn't equal to 8000 or is added to `noised_wav`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "073bc68512dc809aff793dc03dc380d570631457"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['down', 'down', 'down', ..., 'yes', 'yes', 'yes'], dtype='<U5')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_ratio = 0.1\n",
    "noised_wav = []\n",
    "augment = 1\n",
    "delete_index = []\n",
    "for i in range(augment):\n",
    "    new_wav = []\n",
    "    noise = get_one_noise(i)\n",
    "    for i, s in enumerate(wav_all):\n",
    "        if len(s) != 8000:\n",
    "            delete_index.append(i)\n",
    "            continue\n",
    "        s = s + (max_ratio * noise)\n",
    "        noised_wav.append(s)\n",
    "np.delete(wav_all, delete_index)\n",
    "np.delete(label_all, delete_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For later use, wav_vals and label_vals type are changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "6bd94ec47be96cba4be12c60b91ed90044aedea0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21312, 8000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wav_vals = np.array([x for x in wav_all])\n",
    "label_vals = [x for x in label_all]\n",
    "wav_vals.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, a new `label_val` is needed to correspond with the grouping of `wav_all` and `noised_wav` which will occur later. For this purpose, `labels` is duplicated (with copy.deepcopy to create a true copy : if `label_val` change, `labels` will not) and `labels` is added as many times as noise had been added previously.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "94ef3d33c62e27cb1fb53a29025c010ec56ffbb4"
   },
   "outputs": [],
   "source": [
    "labels = copy.deepcopy(label_vals)\n",
    "for _ in range(augment):\n",
    "    label_vals = np.concatenate((label_vals, labels), axis = 0)\n",
    "label_vals = label_vals.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e20c12d950b7ff2e8904fc57d5a9d429cd4b130d"
   },
   "source": [
    "For the moment, only the words on the former list can be detected. However, people may want to say words which aren't in this list or, even, in this language. With the way things are, they will be associated to a word in the list. But it could be better if the model can admit that this word isn't in his dictionnary. To this end, the `unknow` list is composed with a certain number of aleatory chosen audio files of the previously put aside folder (the audio files corresponding to the words of `unknowns_list`). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell creates that `unknow` list. \n",
    "First, it mix up the files so, while taking the 2000*(augment+1) first audio files, every words of `unknowns_list` should appear. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "5ee0b02d7cfbcc02ea20d65ee268ba44e01bf2b1"
   },
   "outputs": [],
   "source": [
    "unknown = unknown_wav\n",
    "np.random.shuffle(unknown_wav)\n",
    "unknown = np.array(unknown)\n",
    "unknown = unknown[:2000*(augment+1)]\n",
    "unknown_label = np.array(['unknown' for _ in range(2000*(augment+1))])\n",
    "unknown_label = unknown_label.reshape(2000*(augment+1),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "72a6f2c8663ee74708da42313ed2e8d24c832b3f"
   },
   "source": [
    "Some audio files of `unknow` may not have the same length than all the previous data so, all the unknow files without the normal length (8000) are deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "70a499bce28fd081b7fa9742eb03851ebe4eb5f1"
   },
   "outputs": [],
   "source": [
    "delete_index = []\n",
    "for i,w in enumerate(unknown):\n",
    "    if len(w) != 8000:\n",
    "        delete_index.append(i)\n",
    "unknown = np.delete(unknown, delete_index, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6a1f8bf0426f373ed063df7f1adbd963943a3fd0"
   },
   "source": [
    "It could be problematic if the algorithm began writing aleatory words of its dictionnary depending on the background sound. To prevent that, a third category of words is created : the `silence_wav`. The same way than before, part of length 8000 of background noise are put in the `silence_wav` list, associated with the label `silence`, which can be found in the `silence_label` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "7ef23bb8580b70f2b52e2d91fb183674112d2eba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3996, 8000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#silence audio\n",
    "silence_wav = []\n",
    "num_wav = (2000*(augment+1))//len(background_noise)\n",
    "for i, _ in enumerate(background_noise):\n",
    "    for _ in range((2000*(augment+1))//len(background_noise)):\n",
    "        silence_wav.append(get_one_noise(i))\n",
    "silence_wav = np.array(silence_wav)\n",
    "silence_label = np.array(['silence' for _ in range(num_wav*len(background_noise))])\n",
    "silence_label = silence_label.reshape(-1,1)\n",
    "silence_wav.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "c06ab04ada253e19a0207d280ed3c1a444776cd3"
   },
   "outputs": [],
   "source": [
    "wav_vals    = np.reshape(wav_vals,    (-1, 8000))\n",
    "noised_wav  = np.reshape(noised_wav,  (-1, 8000))\n",
    "unknown       = np.reshape(unknown,   (-1, 8000))\n",
    "silence_wav = np.reshape(silence_wav, (-1, 8000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4e1666c5bf721dbf9e4717be0d469734db9182f2"
   },
   "source": [
    "For safety, the dimension of the list are checked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "841b9a9b2a2c38297a7a6d1c93744965755703fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21312, 8000)\n",
      "(21312, 8000)\n",
      "(4000, 8000)\n",
      "(3996, 8000)\n"
     ]
    }
   ],
   "source": [
    "print(wav_vals.shape)\n",
    "print(noised_wav.shape)\n",
    "print(unknown.shape)\n",
    "print(silence_wav.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "ab1aba8c05398669f6e3152d74e00e8a41ee7503"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42624, 1)\n",
      "(4000, 1)\n",
      "(3996, 1)\n"
     ]
    }
   ],
   "source": [
    "print(label_vals.shape)\n",
    "print(unknown_label.shape)\n",
    "print(silence_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c6816376d4450daffeb79114df92d3fb0eafb59a"
   },
   "source": [
    "All the different sort of new audio files are added to the list `wav_vals`. `label_vals` is also updated with the corresponding labels. Reminder : the labels corresponding to `noised_wav` have already been added to `label_vals`a little earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "7d6afa30a5dda2b9ca4888b36aabe5b89922a121"
   },
   "outputs": [],
   "source": [
    "wav_vals = np.concatenate((wav_vals, noised_wav), axis = 0)\n",
    "wav_vals = np.concatenate((wav_vals, unknown), axis = 0)\n",
    "wav_vals = np.concatenate((wav_vals, silence_wav), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "420f9d709f805349b2635a80dabc383595eac011"
   },
   "outputs": [],
   "source": [
    "label_vals = np.concatenate((label_vals, unknown_label), axis = 0)\n",
    "label_vals = np.concatenate((label_vals, silence_label), axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before continuing, let's check that `wav_vals` length and `label_vals` length are the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "0c33c9dc3fe0daa57968e89fa6c22352a8b53f55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50620\n",
      "50620\n"
     ]
    }
   ],
   "source": [
    "print(len(wav_vals))\n",
    "print(len(label_vals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "63c2da5df2edd06ab1fbdf6c1a50ad02216f0fe5"
   },
   "source": [
    "### Prepare Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now ready. Like usual, we can now split it in two groups : one for training the model and another one for testing its accuracy.\\\n",
    "The test group, less useful than the training group, will include 20% of the audio files and their corresponding labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "ba0ab85ffd216b4d91855ea234b3202ba250b8be"
   },
   "outputs": [],
   "source": [
    "train_wav, test_wav, train_label, test_label = train_test_split(wav_vals, label_vals, \n",
    "                                                                    test_size=0.2,\n",
    "                                                                    random_state = 1993,\n",
    "                                                                   shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "9e6a9e72fbee7fd3bf612e60d5709b177deacaec"
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "lr = 0.001\n",
    "generations = 20000\n",
    "num_gens_to_wait = 250\n",
    "batch_size = 512\n",
    "drop_out_rate = 0.5\n",
    "input_shape = (8000,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "73ffc6cf54f2100c1aa47203f0c33e350198dc3d"
   },
   "outputs": [],
   "source": [
    "#For Conv1D add Channel\n",
    "train_wav = train_wav.reshape(-1,8000,1)\n",
    "test_wav = test_wav.reshape(-1,8000,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list containing all the labels used is created and called `label_value`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "75fd1a40241009a857b5db9c878c9c665bdc84f2"
   },
   "outputs": [],
   "source": [
    "label_value = target_list\n",
    "label_value.append('unknown')\n",
    "label_value.append('silence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dictionnary allowing to pass from the words to their number is created (`new_label_value`) before being renamed '`label_value`'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "cb801f97812407f60af16a787a0ae01db8e88038"
   },
   "outputs": [],
   "source": [
    "new_label_value = dict()\n",
    "for i, l in enumerate(label_value):\n",
    "    new_label_value[l] = i\n",
    "label_value = new_label_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have a dictionnary, we can switch the labels from being string to being numerate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "65647981d409fb31ac391af3d5d091ef354a0f02"
   },
   "outputs": [],
   "source": [
    "temp = []\n",
    "for v in train_label:\n",
    "    temp.append(label_value[v[0]])\n",
    "train_label = np.array(temp)\n",
    "\n",
    "temp = []\n",
    "for v in test_label:\n",
    "    temp.append(label_value[v[0]])\n",
    "test_label = np.array(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train_label` and `test_label` are converted to two binary class matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = keras.utils.to_categorical(train_label, len(label_value))\n",
    "test_label = keras.utils.to_categorical(test_label, len(label_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using all the matrix created earlier, let's check the dimensions again and verify especially that the lengths of the wav matrix and the label matrix of training and testisting match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "96b340a033294465394c168e232df012a011ae1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Wav Dimension : (40496, 8000, 1)\n"
     ]
    }
   ],
   "source": [
    "print('Train_Wav Dimension : ' + str(np.shape(train_wav)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "994265f130050666438cd7a14357ef766b279495",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Label Dimension : (40496, 12)\n"
     ]
    }
   ],
   "source": [
    "print('Train_Label Dimension : ' + str(np.shape(train_label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "edf649f1ec18a201a7a2b2bf9c5115e443fe6b03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_Wav Dimension : (10124, 8000, 1)\n"
     ]
    }
   ],
   "source": [
    "print('Test_Wav Dimension : ' + str(np.shape(test_wav)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "c3bf3f485f3b3f4e44f25ce4ec35861a7b22f866"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_Label Dimension : (10124, 12)\n"
     ]
    }
   ],
   "source": [
    "print('Test_Label Dimension : ' + str(np.shape(test_label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "557d3f766acfe47197813f0301d0b562cf3388be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Of Labels : 12\n"
     ]
    }
   ],
   "source": [
    "print('Number Of Labels : ' + str(len(label_value)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything is ready, we only need a model now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_uuid": "c3ac90355cbf04993a8b090483164d0e86401855"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/insa/anaconda/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/insa/anaconda/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "input_tensor = Input(shape=(input_shape))\n",
    "\n",
    "x = layers.Conv1D(8, 11, padding='valid', activation='relu', strides=1)(input_tensor)\n",
    "x = layers.MaxPooling1D(2)(x)\n",
    "x = layers.Dropout(drop_out_rate)(x)\n",
    "x = layers.Conv1D(16, 7, padding='valid', activation='relu', strides=1)(x)\n",
    "x = layers.MaxPooling1D(2)(x)\n",
    "x = layers.Dropout(drop_out_rate)(x)\n",
    "x = layers.Conv1D(32, 5, padding='valid', activation='relu', strides=1)(x)\n",
    "x = layers.MaxPooling1D(2)(x)\n",
    "x = layers.Dropout(drop_out_rate)(x)\n",
    "x = layers.Conv1D(64, 5, padding='valid', activation='relu', strides=1)(x)\n",
    "x = layers.MaxPooling1D(2)(x)\n",
    "x = layers.Dropout(drop_out_rate)(x)\n",
    "x = layers.Conv1D(128, 3, padding='valid', activation='relu', strides=1)(x)\n",
    "x = layers.MaxPooling1D(2)(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(256, activation='relu')(x)\n",
    "x = layers.Dropout(drop_out_rate)(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "x = layers.Dropout(drop_out_rate)(x)\n",
    "output_tensor = layers.Dense(len(label_value), activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.Model(input_tensor, output_tensor)\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "             optimizer=keras.optimizers.Adam(lr = lr),\n",
    "             metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_uuid": "0c1c50793d9d2c12af1cbf1029daca221afdaff9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 8000, 1)           0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 7990, 8)           96        \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 3995, 8)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 3995, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 3989, 16)          912       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 1994, 16)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1994, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1990, 32)          2592      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 995, 32)           0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 995, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 991, 64)           10304     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 495, 64)           0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 495, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 493, 128)          24704     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 246, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 31488)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               8061184   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 12)                1548      \n",
      "=================================================================\n",
      "Total params: 8,134,236\n",
      "Trainable params: 8,134,236\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "45ce60c3949727441c5c75dabb3575cb95aa59be"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the train data and test data, the model will be trained to obtain the best performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9b61710f92afdf22a09a07b9dd9d3ddc3e75c05b",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40496 samples, validate on 10124 samples\n",
      "WARNING:tensorflow:From /usr/local/insa/anaconda/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/30\n",
      "40496/40496 [==============================] - 390s 10ms/sample - loss: 2.3790 - acc: 0.1580 - val_loss: 2.3042 - val_acc: 0.2024\n",
      "Epoch 2/30\n",
      "40496/40496 [==============================] - 359s 9ms/sample - loss: 2.1763 - acc: 0.2169 - val_loss: 2.1484 - val_acc: 0.2532\n",
      "Epoch 3/30\n",
      "40496/40496 [==============================] - 354s 9ms/sample - loss: 1.9559 - acc: 0.2951 - val_loss: 1.9051 - val_acc: 0.3413\n",
      "Epoch 4/30\n",
      "28160/40496 [===================>..........] - ETA: 1:42 - loss: 1.7647 - acc: 0.3598"
     ]
    }
   ],
   "source": [
    "model.fit(train_wav, train_label, validation_data=[test_wav, test_label],\n",
    "          batch_size=batch_size, \n",
    "          epochs=30,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5cbb0f06ea603cb0e5ef7b9893080ecfdd082864"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude, this algorithm is able to distinguish the words of its dictionary even if the sounds have a background  noise. However, this algorithm is not able to recognise whole sentences nor homophones. To improve this algorithm, one should have a bigger dictionary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
